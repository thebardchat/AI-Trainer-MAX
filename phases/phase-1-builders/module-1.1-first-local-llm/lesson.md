# Module 1.1: Your First Local LLM

## WHAT YOU'LL BUILD

By the end of this module, you will have a large language model running
on YOUR machine. No internet required. No API key. No monthly bill.
Just your computer, thinking.

Think of it like this: Right now, every time you use ChatGPT, you're
renting someone else's brain. After this module, you OWN the brain.
It lives on your hard drive. It answers to you.

---

## KEY TERMS

**LLM (Large Language Model):** A program trained on massive amounts of
text that can read your questions and generate human-like answers.
It's the engine inside ChatGPT, but you can run one locally.

**Ollama:** A free tool that lets you download and run LLMs on your own
computer. Think of it as the truck that hauls the AI engine to your
garage so you can work on it yourself.

**Model:** The actual AI brain file. Different models have different
sizes and skills. We're using llama3.2:1b — small, fast, and runs
on almost any machine.

**Inference:** When you give the AI a question and it generates an answer.
That's one inference. Every time you hit Enter, that's a job running
on YOUR hardware.

**Parameter count (1b = 1 billion):** How many decision-making connections
the model has. More parameters = smarter but hungrier for RAM. We use 1b
because it runs lean on 7.4GB RAM and still gets real work done.

---

## THE LESSON

### Step 1: Check if Ollama is installed

Open your terminal (CMD or Windows Terminal) and type:

    ollama --version

You should see something like:

    ollama version 0.5.4

If you get "not recognized" — Ollama isn't installed yet.
Go to https://ollama.com and download the Windows installer.
Run it. That's it. No configuration needed.

### Step 2: Start the Ollama server

Ollama runs as a background service. Start it:

    ollama serve

You'll see output like:

    Couldn't find 'C:\Users\YourName\.ollama\id_ed25519'...
    time=2024-01-15T10:00:00.000-06:00 level=INFO msg="inference compute"...
    time=2024-01-15T10:00:00.000-06:00 level=INFO msg="listening on 127.0.0.1:11434"

That last line is the key — Ollama is now listening on port 11434.
Leave this terminal open. Open a NEW terminal for the next steps.

### Step 3: Pull your first model

This downloads the AI brain to your machine. Run:

    ollama pull llama3.2:1b

You'll see a download progress bar. The model is about 1.3GB.
On a decent connection, this takes 2-5 minutes.

When it's done:

    success

That model now lives on your hard drive forever. No subscription.
No expiration date. Yours.

### Step 4: Talk to your AI

    ollama run llama3.2:1b

You'll get a prompt:

    >>> 

Type anything:

    >>> What is a vector database in one sentence?

The model responds right there in your terminal. No internet needed.
That answer was generated by YOUR hardware. That's local inference.

Type /bye to exit the chat.

### Step 5: Use the API (how ShaneBrain talks to Ollama)

Chat mode is nice, but real tools use the API. Try this:

    curl http://localhost:11434/api/generate -d "{\"model\":\"llama3.2:1b\",\"prompt\":\"What is RAM?\",\"stream\":false}"

You'll get a JSON response with the model's answer. This is exactly
how ShaneBrain sends questions to Ollama behind the scenes.

---

## WHAT YOU PROVED

- You can install and run an AI model with zero cloud dependencies
- You understand the difference between renting AI and owning it
- You can talk to a local LLM via chat AND via API
- Your machine can handle inference on a 1b parameter model
- You're ready to give that AI a memory (Module 1.2: Vectors)

---

## NEXT: Run exercise.bat to lock in what you learned.
